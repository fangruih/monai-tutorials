{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEMg3TK0eVPj"
   },
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# 3D regression example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D regression task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to predict the ages of subjects from MR imagee.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_regression/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2pXpPRYeVPl"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T00:20:33.913058Z",
     "start_time": "2024-06-23T00:20:31.451820Z"
    },
    "id": "dlZS78A8eVPl"
   },
   "outputs": [],
   "source": [
    "# !python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l82hea4heVPm"
   },
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T00:20:35.515047Z",
     "start_time": "2024-06-23T00:20:33.921859Z"
    },
    "id": "0mu2D_19eVPm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/simurgh/u/fangruih/miniconda/envs/monai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 1.26.0\n",
      "Pytorch version: 2.3.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /simurgh/u/<username>/miniconda/envs/monai/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.23.2\n",
      "scipy version: 1.14.0\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.17.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.18.1+cu121\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: 1.5.1\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.40.2\n",
      "mlflow version: 2.14.3\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.3rc2\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from monai.networks.nets import Regressor\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVfv5JUVR892"
   },
   "source": [
    "## Setup data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1/abcd/sub-NDARINVJLFDX0WJ/ses-2YearFollowUpYArm1/anat/sub-NDARINVJLFDX0WJ_ses-2YearFollowUpYArm1_run-01_T1w_nrm_crp.npy\n",
      "t1/adni/035_S_0292/MT1__GradWarp__N3m/2013-05-01_14_31_36.0/I371451/ADNI_035_S_0292_MR_MT1__GradWarp__N3m_Br_20130507153329708_S188641_I371451_nrm_crp.npy\n",
      "t1/hcp_aging/HCA6633069_V1_MR/T1w_nrm_crp.npy\n",
      "t1/hcp_dev/HCD0797877_V1_MR/T1w_nrm_crp.npy\n",
      "t1/hcp_ya_mpr1/169343/169343_3T_T1w_MPR1_nrm_crp.npy\n",
      "t1/ppmi/51689/T1-anatomical/2015-12-08_14_41_38.0/I696900/PPMI_51689_MR_T1-anatomical_Br_20160429191726566_S360889_I696900_nrm_crp.npy\n",
      "21051\n",
      "21051\n",
      "6015\n",
      "6015\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "dataset_names=[\"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/abcd/paths_and_info_flexpath.pkl\",\n",
    "               \"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/adni_t1/paths_and_info_flexpath.pkl\",\n",
    "               \"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/hcp_ag_t1/paths_and_info_flexpath.pkl\",\n",
    "               \"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/hcp_dev_t1/paths_and_info_flexpath.pkl\",\n",
    "               \"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/hcp_ya_mpr1/paths_and_info_flexpath.pkl\",\n",
    "               \"/simurgh/u/fangruih/monai-tutorials/generative/3d_ldm/metadata/ppmi_t1/paths_and_info_flexpath.pkl\"]\n",
    "train_images=[]\n",
    "train_ages=[]\n",
    "val_images=[]\n",
    "val_ages=[]\n",
    "for dataset_name in dataset_names:\n",
    "    with open(dataset_name, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        \n",
    "        # Convert paths and ages to lists if they are NumPy arrays\n",
    "        train_new_images = data['train']['paths'].tolist() if isinstance(data['train']['paths'], np.ndarray) else data['train']['paths']\n",
    "        train_new_ages = data['train']['age'].tolist() if isinstance(data['train']['age'], np.ndarray) else data['train']['age']\n",
    "        \n",
    "        val_new_images = data['val']['paths'].tolist() if isinstance(data['val']['paths'], np.ndarray) else data['val']['paths']\n",
    "        val_new_ages = data['val']['age'].tolist() if isinstance(data['val']['age'], np.ndarray) else data['val']['age']\n",
    "        \n",
    "        # Append new data to existing lists\n",
    "        if not train_images:  # More Pythonic way to check if the list is empty\n",
    "            # Direct assignment for the first file\n",
    "            train_images = train_new_images\n",
    "            train_ages = train_new_ages\n",
    "            val_images = val_new_images\n",
    "            val_ages = val_new_ages\n",
    "        else:\n",
    "            # Concatenation for subsequent files\n",
    "            train_images += train_new_images\n",
    "            train_ages += train_new_ages\n",
    "            val_images += val_new_images\n",
    "            val_ages += val_new_ages\n",
    "        \n",
    "        # Debug output to check the results\n",
    "        print(train_images[-1])  # Print the last path\n",
    "        \n",
    "prefix = \"/scr/fangruih/stru/\"\n",
    "train_images = [prefix + train_image for train_image in train_images]\n",
    "val_images = [prefix + val_image for val_image in val_images]\n",
    "\n",
    "print(len(train_images))  # Print the total number of paths loaded\n",
    "print(len(train_ages))  # Print the total number of paths loaded\n",
    "\n",
    "print(len(val_images))  # Print the total number of paths loaded\n",
    "print(len(val_ages))  # Print the total number of paths loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the loaded data: (1, 160, 192, 176)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the .npy file\n",
    "file_path = \"/scr/fangruih/stru/t1/hcp_ya_mpr1/169343/169343_3T_T1w_MPR1_nrm_crp.npy\"\n",
    "# Load the numpy file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Print the dimensions of the loaded data\n",
    "print(\"Dimensions of the loaded data:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3-MIx_lSNKF"
   },
   "source": [
    "## Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yVvt3XtHeVPn",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'monai.data.meta_tensor.MetaTensor'> torch.Size([3, 1, 160, 192, 176]) tensor([13.4167, 11.2500,  9.5000], dtype=torch.float64) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "# train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(channel_dim=0), Resize((148,180,148)), RandRotate90()])\n",
    "train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(channel_dim=0), Resize((160, 192, 176))])\n",
    "val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(channel_dim=0),Resize((160, 192, 176))])\n",
    "# train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(channel_dim=0), Resize((148,180,148))])\n",
    "# val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(channel_dim=0),Resize((148,180,148))])\n",
    "\n",
    "# train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96)), RandRotate90()])\n",
    "# val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=train_images, labels=train_ages, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=train_images, labels=train_ages, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=val_images, labels=val_ages, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal loss (basically more weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossRegression(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLossRegression, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute the absolute error per example\n",
    "        error = torch.abs(inputs - targets)\n",
    "        \n",
    "        # Compute the modulating factor\n",
    "        modulating_factor = torch.pow(error, self.gamma)\n",
    "        \n",
    "        # Compute the focal loss\n",
    "        loss = modulating_factor * error  # This is equivalent to error^(gamma + 1)\n",
    "        \n",
    "        # Apply reduction method\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss  # No reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgH8asdqSaiT"
   },
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l9zwee86) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-breeze-18</strong> at: <a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor/runs/l9zwee86' target=\"_blank\">https://wandb.ai/fangruih-Stanford%20University/age-regressor/runs/l9zwee86</a><br/> View project at: <a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor' target=\"_blank\">https://wandb.ai/fangruih-Stanford%20University/age-regressor</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240928_145159-l9zwee86/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l9zwee86). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/simurgh/u/fangruih/monai-tutorials/3d_regression/wandb/run-20240928_145306-a01aeup4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor/runs/a01aeup4' target=\"_blank\">jolly-frost-19</a></strong> to <a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor' target=\"_blank\">https://wandb.ai/fangruih-Stanford%20University/age-regressor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fangruih-Stanford%20University/age-regressor/runs/a01aeup4' target=\"_blank\">https://wandb.ai/fangruih-Stanford%20University/age-regressor/runs/a01aeup4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "100/10525, train_loss: 11440.7812\n",
      "200/10525, train_loss: 95778.8516\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from model import Regressor  # Assuming this is your model's import statement.# Initialize wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current time\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "wandb.init(project=\"age-regressor\")\n",
    "\n",
    "# Setup the model\n",
    "# model = Regressor(in_shape=[1,148,180,148], out_shape=1, channels=(16, 32, 64, 128, 256), strides=(2, 2, 2, 2))\n",
    "model = Regressor(in_shape=[1,160, 192, 176], out_shape=1, channels=(16, 32, 64, 128, 256), strides=(2, 2, 2, 2))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Loss and optimizer\n",
    "# loss_function = MSELoss()\n",
    "loss_function = FocalLossRegression(gamma=2.0, reduction='mean')\n",
    "\n",
    "optimizer = Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# Training settings\n",
    "val_interval = 2\n",
    "max_epochs = 100\n",
    "best_metric = sys.float_info.max\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        if step %100==0:\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Log training loss every 100 stepsif step % 100 == 0:\n",
    "    wandb.log({\"train_loss\": loss.item(), \"step\": epoch * len(train_loader) + step})\n",
    "\n",
    "    # Average loss for epoch\n",
    "    epoch_loss /= len(train_loader)\n",
    "    wandb.log({\"epoch_loss\": epoch_loss, \"epoch\": epoch})\n",
    "\n",
    "    # Validationif (epoch + 1) % val_interval == 0:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = loss_function(val_outputs, val_labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        wandb.log({\"val_loss\": avg_val_loss, \"epoch\": epoch})\n",
    "\n",
    "        # Check if this is the best modelif avg_val_loss < best_metric:\n",
    "        best_metric = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"trained_model/best_metric_model_{current_time}.pth\")\n",
    "        print(\"Saved new best model with loss:\", best_metric)\n",
    "\n",
    "print(\"Training completed. Best validation loss:\", best_metric)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrwW4TLneVPp"
   },
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
